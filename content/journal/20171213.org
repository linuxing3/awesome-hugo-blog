* Wednesday, 12/13/2017


** Colly provides a clean interface to write any kind of crawler/scraper/spider

With Colly you can easily extract structured data from websites, which can be
used for a wide range of applications, like data mining, data processing or
archiving.

** Features
*** Clean API
*** Fast (>1k request/sec on a single core)
*** Manages request delays and maximum concurrency per domain
*** Automatic cookie and session handling
*** Sync/async/parallel scraping
*** Caching
*** Automatic encoding of non-unicode responses
*** Robots.txt support
*** Google App Engine support

** example


#+begin_src go

package main

import (
	"encoding/json"
	"fmt"
	"log"
	"strings"

	"github.com/gocolly/colly"
)

// Course stores information about a coursera course
type Course struct {
	Title string
	URL   string
	text  string
}

func main() {
	// Instantiate default collector
	c := colly.NewCollector()

	// Visit only domains: coursera.org, www.coursera.org
	c.AllowedDomains = []string{"eluniversal.com", "www.eluniversal.com"}

	// Cache responses to prevent multiple download of pages
	// even if the collector is restarted
	c.CacheDir = "./eluniversal_cache"

	// Create another collector to scrape course details
	detailCollector := c.Clone()

	courses := make([]Course, 0, 200)

	// Before making a request print "Visiting ..."
	c.OnRequest(func(r *colly.Request) {
		log.Println("visiting", r.URL.String())
	})

	// On every a HTML element which has name attribute call callback
	c.OnHTML(`a`, func(e *colly.HTMLElement) {
		// Activate detailCollector if the link contains "coursera.org/learn"
		newsURL := e.Request.AbsoluteURL(e.Attr("href"))
		if strings.Index(newsURL, "eluniversal.com/noticias") != -1 {
			detailCollector.Visit(newsURL)
		}
	})

	// Extract details of the course
	detailCollector.OnHTML(`h2[class=title]`, func(e *colly.HTMLElement) {

		log.Println("News found", e.Request.URL)

		title := e.Text
		if title == "" {
			log.Println("No title found", e.Request.URL)
		}

		course := Course{
			Title: title,
			URL:   e.Request.URL.String(),
		}
		// Iterate over rows of the table which contains different information
		// about the course

		courses = append(courses, course)
	})

	// Start scraping on http://coursera.com/browse
	c.Visit("http://eluniversal.com")

	// Convert results to JSON data if the scraping job has finished
	jsonData, err := json.MarshalIndent(courses, "", "  ")
	if err != nil {
		panic(err)
	}

	// Dump json to the standard output (can be redirected to a file)
	fmt.Println(string(jsonData))
}
#+end_src
